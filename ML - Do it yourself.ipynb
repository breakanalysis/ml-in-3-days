{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML - Do it yourself\n",
    "In this notebook we will get a feel for data generation and a few models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distribution\n",
    "- Use np.random.randn to sample 1000 numbers independently from the N(0,1) distribution. This means a random normal (or gaussian) variable with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "- Use np.histogram to compute histogram for x using bins [-3,-2] , [-2,1], ... , [2,3].\n",
    "- Use df.hist for the same reason (bonus = plot).\n",
    "- Use plt.hist to plot histogram of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate gaussian\n",
    "- Use np.random.randn to sample 1000 two-dimensional vectors independently from the N(0,1) distribution.\n",
    "- Mean value should be (0,0), and the two components of the vector should be independent. Also each sample should be independent.\n",
    "\n",
    "(The second requirement is true by default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot\n",
    "- Split twoD into two arrays representing the two dimensions.\n",
    "- Use plt.scatter to draw the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = # your code here\n",
    "y = # your code here\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and translate\n",
    "- Transform twoD by scaling both axes by 2.\n",
    "- Transform twoD by adding 5 to x and subtracting 5 from y.\n",
    "- Compute mean (along axis 0) and standard deviation as a sanity check. Means should be close to [1,-1] and standard deviations should be close to [2,2].\n",
    "- Draw the scatter plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD # your code here\n",
    "twoD[# fill in ] += 5\n",
    "twoD[# fill in ] -= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append more rows\n",
    "- Sample another 1000 samples of 2D gaussian distribution, again N(0,1) independent entries.\n",
    "- Create a concatenated 2D array with twoD followed by the additional samples. For this use np.vstack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD2 = # your code here\n",
    "X = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == (2000,2)\n",
    "assert twoD2.shape == (1000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels\n",
    "- Make a numpy array with shape (2000,1): 1000 entries of 1 followed by 1000 entries of -1. For this use np.ones and np.vstack. Note that np.ones will need a tuple or similar as argument. (By the way np.zeros is a similar function).\n",
    "- Stack this to the right of X to obtain a (2000,3) shaped array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = # your code here\n",
    "data = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.vstack([np.ones((1000,1)),-1*np.ones((1000,1))])\n",
    "data = np.hstack([X, Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Y.shape == (2000,1)\n",
    "assert Y.sum() == 0\n",
    "assert Y[0] == 1\n",
    "assert data.shape == (2000,3)\n",
    "assert data[:,2].sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "- The first 1000 rows with gray. Color is set by the named argument c.\n",
    "- The remaining rows with pink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the rows\n",
    "- Use np.random.permutation to create a random index to shuffle the rows with.\n",
    "- Use indexing to create a shuffled version of the data.\n",
    "- Do the same using np.random.shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = # your code here\n",
    "data = # your code here\n",
    "data = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data[:,2].sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "Sklearn, or scikit-learn, is a very popular, lightweight and easy to use library for machine learning.\n",
    "It allows you to train models using a few lines of code but still is flexible for extension, varying algorithms, metrics, loss-functions and other hyper parameters.\n",
    "\n",
    "We will train a logistic regression model for the same data set as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algorithms, or Inducers, in sklearn have few but important methods. Take a look at the help for 'fit' and 'predict'. Also if you type reg.<tab\\> you will see what methods are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Fit the logistic regression model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg.fit(data[:,:-1], data[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the values of hyperparameters used above. We can also programatically access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights can be accessed like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = reg.coef_[0]\n",
    "c = reg.intercept_[0]\n",
    "(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "We use the same code as above to plot the logistic regression decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(twoD2[:,0], twoD2[:,1], c='pink')\n",
    "plt.scatter(twoD[:,0], twoD[:,1], c='gray')\n",
    "ticks = [-4 + 0.16*t for t in range(100)]\n",
    "boundary_y = [-(a*x + c)/b for x in ticks]\n",
    "boundary_y = [b if b < 15 else 15 for b in boundary_y]\n",
    "boundary_y = [b if b > -15 else -15 for b in boundary_y]\n",
    "gray = 1-epoch/epochs\n",
    "plt.plot(ticks, boundary_y, label='epoch: ' + str(epoch), c = (gray,gray,gray))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Get the model's predictions for the entire data set.\n",
    "- Compute the accuracy of the classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = # your code here\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Perform 5-fold cross validation with logistic regression by using cross_val_score.\n",
    "- The evaluation metric is accuracy.\n",
    "- Only 1 line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other metrics see\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Retrain the logistic regression model above but use sklearn's validation_curve method to get training and validation scores over training iterations. Specify train_sizes=np.linspace(0.1, 1.0, 200) and use 5 fold cross validation.\n",
    "- Average the output scores over folds.\n",
    "- Plot train scores and test scores in the same plot.\n",
    "- Repeat the above for \"neg_log_loss\" instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes, train_scores, test_scores = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = # your code here\n",
    "test_scores_mean = # your code here\n",
    "\n",
    "plt.plot(sizes, -train_scores_mean, c='red', label='train')\n",
    "plt.plot(sizes, -test_scores_mean, c='blue', label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for negative log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Use np.logspace to create a list of values _params_ $=10^{-4},10^{-3},10^{-2},10^{-1},10^{0},10^{1}$.\n",
    "- Use validation_curve to train a logistic regression model on 'digits' data. The parameter C should take on the values above. Use 5-fold crossvalidation and negative log loss. However when plotting use plt.semilogx instead of plt.plot.\n",
    "- Take averages over folds and plot average train loss and average validation loss vs _params_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loads hand written character image data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "params = # your code here\n",
    "train_scores, valid_scores = # your code here\n",
    "train_scores_mean = # your code here\n",
    "valid_scores_mean = # your code here\n",
    "\n",
    "assert train_scores_mean.shape == (6,)\n",
    "assert valid_scores_mean.shape == (6,)\n",
    "\n",
    "plt.semilogx( # your code here)\n",
    "plt.semilogx( # your code here)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.set_params(penalty='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain\n",
    "Now rerun your cell above and check the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Read the help on GridSearchCV.\n",
    "- Set parameters correctly so that\n",
    "    - when _kernel_ is 'poly' then _degree_ takes the values 1,2 and 3\n",
    "    - when _kernel_ is 'rbf', then _C_ takes values 1 and 10\n",
    "    - when _kernel_ is 'linear', then _C_ and _gamma_ take on all combinations of \\[1,10\\] and \\[0.1,1\\].\n",
    "- Run the cell to fit a lot of models to iris data or digits data!\n",
    "- Study the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "parameters = # your code here\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "clf.fit(iris.data, iris.target)\n",
    "#clf.fit(digits.data, digits.target)\n",
    "df = pd.DataFrame(clf.cv_results_)\n",
    "# should be 9 parameter settings\n",
    "assert df.shape[0] == 9 \n",
    "df.sort_values('mean_test_score', ascending=False, inplace=True)\n",
    "important_cols = [col for col in df.columns.values if not 'split' in col]\n",
    "df[important_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional material\n",
    "The exercises below are optional for those that want to get into the details and math of an algorithm based on gradient descent. It is possible to do machine learning by relying on libraries for the low-level mathematical algorithms, and in fact this the typical method in applying machine learning.\n",
    "\n",
    "However, it may be interesting to have a feel of how such an algorithm may work in order to demystify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with without a lib\n",
    "For the sport of it we will do our own version of gradient descent and see if we can learn something!\n",
    "\n",
    "First off, we will use the data set we just created and build a linear binary classifier for it.\n",
    "That is, $h(x,y) = ax + by + c$.\n",
    "\n",
    "The last term is called the bias term. Note that the classifier is a dot-product of vectors:\n",
    "\n",
    "$h(x,y) = w \\cdot X$, where\n",
    "\n",
    "$w = [a,b,c]$, and\n",
    "\n",
    "$X = [x,y,1]$.\n",
    "\n",
    "Thus we have three things at play, the hypothesis $h$, its weight vector $w$, and an augmented feature vector $X$.\n",
    "Note that implementing the bias by adding this last 'always-on' feature to $X$ makes the formula for $h$ very elegant.\n",
    "\n",
    "Now imagine we have a loss function $L$, but we haven't decided quite yet which loss function we want.\n",
    "For each example $(X,c)$, where $c$ is the target, the loss for that example is $L = L(h(X),c)$. We can see this as having one loss-function for $c=-1$ and one for $c=1$, but for this example we are using one of these. To simplify further and lets assume $L = L(ch(X))$.\n",
    "\n",
    "Let's differentiate one of these with respect to one of the weights $w_i$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = L'(ch(X)) \\frac{\\partial ch(X)}{\\partial w_i} = L'(ch(X)) \\frac{\\partial (w \\cdot X)}{\\partial w_i} = cL'(ch(X))X_i$$.\n",
    "\n",
    "Putting the three partial derivatives in one vector gives us the gradient of the loss\n",
    "\n",
    "$$\\nabla_w L = cL'(ch(X))X$$.\n",
    "\n",
    "Let's pause and examine. The number $cL'(ch(X))$, whatever it is can be seen as a weight or importance of the example X. What gradient descent now wants to do is add $\\nabla_w L$ to the weight vector $w$. So some conclusions:\n",
    "\n",
    "- The update to $w$ is a linear combination of all the examples $X$.\n",
    "- In general $w = w_0 + X^T W$, that is, a (small) start value plus a linear combination of the training examples.\n",
    "- To compute the gradient, we don't need a formula for $L$, we can design $L'$ directly!\n",
    "\n",
    "In general it is better to have a loss function that focuses on the biggest mistakes. An idea we can try is to let\n",
    "$$L'(h(X),c) = e^{-ch(X)}$$.\n",
    "\n",
    "So if $c=1$, the examples with lowest $h(X)$ will be seen as the most important mistakes, and for $c=-1$, the training will focus on examples with highest $h(X)$.\n",
    "\n",
    "We have to apply one trick. This loss will strongly focus on the single or few biggest mistakes which makes learning unstable and noisy. A remedy is to take the median of $L'$ over all examples and truncate $L'$ at a maximum of the median value.\n",
    "\n",
    "Finally we will normalize the example weights so that they sum to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Define the derivative of L as a function to be the exponential function above. Use np.exp.\n",
    "- Note that we want L' and not cL'.\n",
    "- The function has only one argument s, which means the caller should provide the value $s=ch(X)$.\n",
    "- Name the function dL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Define the hypothesis as a function h(X,w).\n",
    "- The weight vector w has shape (3,).\n",
    "- The input X is a numpy array with shape (N,3).\n",
    "- The output is a numpy array with shape (N,).\n",
    "\n",
    "Comment: the output represents the classifier's score for all of the $N$ examples.\n",
    "\n",
    "Hint: You can use matrix multiplication or indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Create X by copying data.\n",
    "- Replace the target column's values by all 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Fill in the gaps in the fit method below, indicated by tripple comments ###."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate, try different values if you want. What is the connection to epochs?\n",
    "lr = 1.5\n",
    "# Epochs is the number of passes through the data and also the number of weight updates.\n",
    "epochs = 20\n",
    "\n",
    "# x values for plotting\n",
    "ticks = [-4 + 0.16*t for t in range(100)]\n",
    "\n",
    "# plot data \n",
    "plt.scatter(twoD2[:,0], twoD2[:,1], c='pink')\n",
    "plt.scatter(twoD[:,0], twoD[:,1], c='gray')\n",
    "\n",
    "# initialization of weights\n",
    "w = np.random.randn(3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # compute gradient\n",
    "    scores = ### compute the scores of all examples\n",
    "    sign_scores = ### multiply each score with the value of its class c \n",
    "    acc = ### compute accuracy of the classifier, assuming the output of the classifier is 1 if h(X)>0 and -1 otherwise\n",
    "    print('accuracy for epoch %s is %s' % (epoch, acc))\n",
    "    example_weights = ### compute raw positive weights for each example using the derivative of the loss function\n",
    "    median = np.median(example_weights)\n",
    "    example_weights = ### using list comprehension or np.where, replace all values higher than median with median\n",
    "    example_weights /= ### divide by constant to make sum equal 1\n",
    "    example_weights = ### multiply by the c values to get the correct direction for the update\n",
    "    grad = ### compute the gradient using matrix multiplication\n",
    "    # update weights\n",
    "    w += lr*grad\n",
    "    a, b, c = w\n",
    "    # y-values of the decision boundary line\n",
    "    boundary_y = [-(a*x + c)/b for x in ticks]\n",
    "    boundary_y = [b if b < 15 else 15 for b in boundary_y]\n",
    "    boundary_y = [b if b > -15 else -15 for b in boundary_y]\n",
    "    gray = 1-epoch/epochs\n",
    "    plt.plot(ticks, boundary_y, label='epoch: ' + str(epoch), c = (gray,gray,gray))\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
